{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation -- Humans and Animals\n",
    "#### Thomas Reynolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "from matplotlib import patches\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import pycocotools\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "import albumentations as A\n",
    "from random import sample\n",
    "import splitfolders\n",
    "from os import path\n",
    "\n",
    "# https://github.com/pytorch/vision/tree/main/references/detection\n",
    "import sys\n",
    "sys.path.append('./dependencies')\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the keypoints and bounding box for some images\n",
    "for i in sample(range(1,3000),5):\n",
    "\n",
    "    # Read in keypoints data\n",
    "    with open('data/annotations.json') as f:\n",
    "        img_info = json.load(f)\n",
    "\n",
    "    # Get an image\n",
    "    img_path = 'data/train/images/' + str(img_info['images'][str(i)])\n",
    "    \n",
    "    # check if exists otherwise it's in test set\n",
    "    if path.exists(img_path):\n",
    "        test_img = image.imread(img_path)\n",
    "\n",
    "        # Get the keypoints for the image\n",
    "        key_data_path = 'data/train/annotations/' + img_path[18:-4] + '.json'\n",
    "        with open(key_data_path, 'r') as f:\n",
    "            key_data = json.loads(f.read())\n",
    "    \n",
    "    else:\n",
    "        img_path = 'data/test/images/' + str(img_info['images'][str(i)])\n",
    "\n",
    "        test_img = image.imread(img_path)\n",
    "\n",
    "        # Get the keypoints for the image\n",
    "        key_data_path = 'data/test/annotations/' + img_path[17:-4] + '.json'\n",
    "        with open(key_data_path, 'r') as f:\n",
    "            key_data = json.loads(f.read())\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Annotate over image\n",
    "    for object in key_data['keypoints']:\n",
    "        for point in object:\n",
    "            if point[2] != 0:\n",
    "                # do stuff\n",
    "                plt.plot(point[0], point[1], marker='.', color=\"white\")\n",
    "\n",
    "    plt.imshow(test_img)\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    rects = []\n",
    "    for bbox in key_data['bboxes']:\n",
    "        rects.append(patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], linewidth=1, edgecolor='r', facecolor='none'))\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    for rect in rects:\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            bboxes_labels_original = ['Detected Object' for _ in bboxes_original]            \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            \n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            if len(transformed['keypoints']) % 20 == 0:\n",
    "                keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,20,2)).tolist()\n",
    "            else:\n",
    "                while len(transformed['keypoints']) % 20 != 0:\n",
    "                    transformed['keypoints'].append([0,0])\n",
    "                keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,20,2)).tolist()\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    # kp - coordinates of keypoint\n",
    "                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)        \n",
    "        img = F.to_tensor(img)\n",
    "        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassDataset('data/train', transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "print(\"Transformed targets:\\n\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some images\n",
    "\n",
    "keypoints_classes_ids2names = {0:\"left_eye\", 1:\"right_eye\", 2:\"nose\", 3:\"left_ear\", 4:\"right_ear\", 5:\"left_front_elbow\",\n",
    "    6:\"right_front_elbow\", 7:\"left_back_elbow\", 8:\"right_back_elbow\", 9:\"left_front_knee\", 10:\"right_front_knee\",\n",
    "    11:\"left_back_knee\", 12:\"right_back_knee\", 13:\"left_front_foot\", 14:\"right_front_foot\", 15:\"left_back_foot\",\n",
    "    16:\"right_back_foot\", 17:\"throat\", 18:\"back\", 19:\"tailbase\"}\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 18\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 1, (255,0,0), 10)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(40,40))\n",
    "        plt.imshow(image)\n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "        \n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 1, (255,0,0), 10)\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = 'data/train'\n",
    "KEYPOINTS_FOLDER_TEST = 'data/test'\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model(num_keypoints = 20)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_test, device)\n",
    "    \n",
    "# Save model weights after training\n",
    "torch.save(model.state_dict(), 'keypointsrcnn_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(data_loader_test)\n",
    "images, targets = next(iterator)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(images)\n",
    "\n",
    "print(\"Predictions: \\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
